{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow tutorial 1: Models and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't panic, read the docs (https://www.tensorflow.org/beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Classifying hand-written digits / MNIST dataset\n",
    "Contents:\n",
    "- `tf.keras.models.Sequential`\n",
    "- `tf.keras.layers.*`\n",
    "\n",
    "- Obtain data\n",
    "- Constructing a model\n",
    "- Choose a loss function\n",
    "- Training\n",
    "- Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "x_test = np.array(x_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First dimension identifies sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulate a model $f_w(x)$ and attach loss function and observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use the __Sequential__ API here, i.e. we define the computation graph when `model` is constructed. Tensorflow (Keras) also supports a __functional__ API (https://www.tensorflow.org/beta/guide/keras/functional) in which you procedurally build the graph (potentially with branches), and then construct a model from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is randomly initialized, applying the forward pass to the image of the five will not yield a reliable prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.apply(x_train[0:1])\n",
    "plt.bar(range(0,10), prediction[0])\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(\"Assignment probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model again to the image of the five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.apply(x_train[0:1])\n",
    "plt.bar(range(0,10), prediction[0])\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(\"Assignment probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interestingly we can evaluate the performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) MNIST using convolutional layers\n",
    "Contents:\n",
    "- functional api to construct models, `tf.keras.Input`\n",
    "- `tf.keras.layers.Conv2D`\n",
    "- `tf.keras.optimizers`\n",
    "- `tf.keras.losses`\n",
    "- `tf.keras.metrics`\n",
    "- `tf.data.Dataset`\n",
    "\n",
    "- Constructing a model (convolutions better suited for images compared to dense)\n",
    "- Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "image taken from wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a convolutional layer with 32 filters, each one with a size 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_api = True\n",
    "if not functional_api:\n",
    "    conv_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "else:\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    x = tf.keras.layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    conv_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "conv_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the `tf.data.Dataset`, which generalizes data coming from different sources. Here we will simply consume our `np.ndarray` objects. The `Dataset` supports functions like [`apply`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#apply), [`batch`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#batch), [`shuffle`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle), [`map`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the dataset from numpy arrays\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Add channel dimension\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.expand_dims(x, -1), y))\n",
    "test_dataset= test_dataset.map(lambda x, y: (tf.expand_dims(x, -1), y))\n",
    "\n",
    "# which on direct numpy data would be\n",
    "x_train2 = x_train[..., np.newaxis]\n",
    "x_test2 = x_test[..., np.newaxis]\n",
    "\n",
    "# Batch data\n",
    "train_dataset = train_dataset.shuffle(len(x_train)).batch(64)\n",
    "test_dataset= test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(train_dataset.__iter__())\n",
    "prediction = conv_model.apply(image[0:1])\n",
    "plt.bar(range(0,10), prediction[0])\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(\"Assignment probability\")\n",
    "print(\"Label\", label[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Linear regression, example Hooke's law of a two-dimensional oscillator\n",
    "- subclass `tf.keras.Model`\n",
    "- `tf.keras.Model.save_weights`\n",
    "- `tf.keras.Model.load_weights`\n",
    "- Constructing a (linear) model\n",
    "\n",
    "Given a point mass is attached to two springs such that it can oscillate in two orthogonal directions  independently. Given measurements of the force on the point mass at given excitations $x$, we want to find a model that predicts the force from $x$. Our model for the resetting force $F\\in\\mathbb{R}^2$ is a linear dependence on the displacement vector $x\\in\\mathbb{R}^2$, i.e. $F(x)=Wx$, where $W\\in\\mathbb{R}^{2\\times2}$ is a matrix that contains the spring constants. (Model is both linear in $x$ and $W$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively try the '2d-hooke-rotated.npz'\n",
    "with np.load(\"2d-hooke.npz\") as file:\n",
    "    xs, ys = file[\"xs\"], file[\"ys\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist2d(xs[:,0], xs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist2d(ys[:,0], ys[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect correlations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df = pd.DataFrame({\"x1\": xs[:,0], \"x2\": xs[:,1], \"y1\": ys[:,0], \"y2\": ys[:,1]})\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=len(xs)\n",
    "n_test = n_samples // 10\n",
    "n_train = n_samples - n_test\n",
    "n_samples, n_train, n_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will formulate our own model, which here consists out of some trainable parameters (contained in `self.dense`) and an implementation of the forward pass `call(self, inputs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(2, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize the model to fit our data, we have to specify an optimization criterion. In our case we aim to minimize the mean squared error of our model $F_W=f_w$ on the observed data $(x_n, y_n), ~ n \\in [N]$ with respect to the parameters $w$:\n",
    "\n",
    "$$w_{*} = \\arg\\min_{w} \\tfrac{1}{N} \\sum_{n=1}^{N}(f_{w}(x_i) - y_i)^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-2),\n",
    "              loss=tf.keras.losses.mean_squared_error,\n",
    "              metrics=[tf.keras.metrics.mean_squared_error])\n",
    "model.build((None, 2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(xs[:n_train], ys[:n_train], epochs=200,\n",
    "          validation_data=(xs[n_train:], ys[n_train:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics[0].result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dense.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(-0.5,0.5,n)\n",
    "y = np.linspace(-0.5,0.5,n)\n",
    "X, Y = np.meshgrid(x,y)\n",
    "\n",
    "samples = np.stack((X,Y), axis=-1)\n",
    "samples_flat = np.reshape(samples, (n*n, 2))\n",
    "res_flat = model.apply(samples_flat)\n",
    "res = np.reshape(res_flat, (n,n,2))\n",
    "z1 = res[..., 0]\n",
    "z2 = res[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcol = plt.pcolor(X, Y, z1, vmin=-2.0,vmax=2.0)\n",
    "plt.contour(X, Y, z1, colors=\"white\", linestyles=\"-\")\n",
    "cbar = plt.colorbar(pcol)\n",
    "cbar.ax.set_ylabel(r\"Prediction $y_1$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcol = plt.pcolor(X, Y, z2, vmin=-2.0,vmax=2.0)\n",
    "plt.contour(X, Y, z2, colors=\"white\", linestyles=\"-\")\n",
    "cbar = plt.colorbar(pcol)\n",
    "cbar.ax.set_ylabel(r\"Prediction $y_2$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the weights to file for later usage. In particular this will write a single [checkpoint](https://www.tensorflow.org/beta/guide/checkpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./hooke-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls | grep hooke-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = MyModel()\n",
    "loaded.build((None, 2))\n",
    "loaded.load_weights(\"./hooke-model\")\n",
    "loaded.dense.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Overfitting\n",
    "- Checkpointing with `keras.callbacks.ModelCheckpoint`\n",
    "- Visualize training metrics using `history`\n",
    "\n",
    "In the previous example we knew exactly how many parameters are required to reconstruct the given data (up to noise). In the general case, the optimal complexity of a model is not known _a priori_.\n",
    "\n",
    "To demonstrate this let's use a model with much more parameters than there are datapoints. In other words, the model is much too expressive for the problem and the amount of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./overfit_ckpts\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(checkpoint_dir, \"of-model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train[::1000], y_train[::1000], epochs=40, validation_data=(x_test, y_test), \n",
    "                    callbacks=[model_checkpoint], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8,3))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(history.epoch, history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "#plt.yscale(\"log\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(history.epoch, history.history[\"sparse_categorical_accuracy\"],\n",
    "         label=\"train accuracy\")\n",
    "plt.plot(history.epoch, history.history[\"val_sparse_categorical_accuracy\"], \n",
    "         label=\"val accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the performance on the training set always improves, the performance on \"unseen\" data declines after some time. This indicates __overfitting__. The model is too complex/expressive/rich in parameters and can learn the training set by heart. Hence it also occurs when you have too few data.\n",
    "\n",
    "The textbook analogy is doing a polynomial fit, with as many trainable parameters as datapoints.\n",
    "\n",
    "<img src=https://qph.fs.quoracdn.net/main-qimg-28d4d605380ee139f5079e18bacdf630 width=\"300\">\n",
    "image taken from quora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally overfitting is avoided by choosing a simpler model.\n",
    "### 5) Underfitting\n",
    "Let's consider a very simple model with few parameters. In other words, a model that is not expressive enough to fulfill the desired function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(8,8), input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train2, y_train, epochs=10, validation_data=(x_test2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8,3))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(history.epoch, history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "#plt.yscale(\"log\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(history.epoch, history.history[\"sparse_categorical_accuracy\"],\n",
    "         label=\"train accuracy\")\n",
    "plt.plot(history.epoch, history.history[\"val_sparse_categorical_accuracy\"], \n",
    "         label=\"val accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we certainly do not overfit, but we cannot even make good predictions on the training data. This is __underfitting__. The model we chose has too little complexity/is too strongly biased.\n",
    "\n",
    "E.g. consider the prediction of the digit 'five':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(10), model.apply(x_train2[0:1]).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Regularization\n",
    "- `tf.train.Checkpoint`\n",
    "- write own training procedure\n",
    "- eager execution\n",
    "\n",
    "Oftentimes it is not clear, which simple model is the right one, i.e. the optimal model bias is not known. Starting from a complex model, __regularization__ prevents overfitting by introducing a systematic bias. Most important regularization methods are:\n",
    "- Dropout\n",
    "- Early stopping (`tf.keras.callbacks.EarlyStopping`)\n",
    "- L1 or L2 penalty on parameters\n",
    "\n",
    "Going back to the very first MNIST model, where we used dropout.\n",
    "\n",
    "Note that we will now write our own training procedure which enables us to use tensorflows `tf.train.Checkpoint` and `tf.train.CheckpointManager` (without relying on keras' callback specification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.4),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather `model`, `optimizer`, `losses` and `metrics`. Note that all these objects are _stateful_. E.g.`SparseCategoricalCrossentropy` is not simply a function but an object that computes and stores the result. This is why we need one metric for each: training loss and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((64, 28, 28, 1))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='val_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_sparse_categorical_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "    \"\"\"\n",
    "    Predicts the output of `images`, calculates and applies gradients to model parameters.\n",
    "    Also calculates train metrics\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # calculate metrics\n",
    "    train_accuracy(labels, predictions)\n",
    "    train_loss(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(images, labels):\n",
    "    \"\"\"Evaluates prediction on given data and calculates test metrics\"\"\"\n",
    "    predictions = model(images, training=False)\n",
    "    # calculate metrics\n",
    "    test_loss(labels, predictions)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./regularization_ckpts/\"\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=None)\n",
    "status = checkpoint.restore(manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poor man's history\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"train_accuracy\": [], \"test_loss\": [], \"test_accuracy\": []}\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    ckpt_path = manager.save()\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, ckpt {}'\n",
    "    print(template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         tf.round(train_accuracy.result()*1000),\n",
    "                         test_loss.result(),\n",
    "                         tf.round(test_accuracy.result()*1000),\n",
    "                         ckpt_path))\n",
    "    \n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss.result())\n",
    "    history[\"train_accuracy\"].append(train_accuracy.result())\n",
    "    history[\"test_loss\"].append(test_loss.result())\n",
    "    history[\"test_accuracy\"].append(test_accuracy.result())\n",
    "    \n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(train_dataset.__iter__())\n",
    "prediction = model(image[0:1], training=False)\n",
    "plt.bar(range(0,10), prediction[0])\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(\"Assignment probability\")\n",
    "print(\"Label\", label[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(manager.checkpoints[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(train_dataset.__iter__())\n",
    "prediction = model.apply(image[0:1])\n",
    "plt.bar(range(0,10), prediction[0])\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(\"Assignment probability\")\n",
    "print(\"Label\", label[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8,3))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train loss\")\n",
    "plt.plot(history[\"epoch\"], history[\"test_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "#plt.yscale(\"log\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(history[\"epoch\"], history[\"train_accuracy\"],\n",
    "         label=\"train accuracy\")\n",
    "plt.plot(history[\"epoch\"], history[\"test_accuracy\"], \n",
    "         label=\"val accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) MNIST Autoencoder, fill-in-the-blank exercise\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/220px-Autoencoder_schema.png)\n",
    "(wikipedia)\n",
    "\n",
    "We now want to learn a compressed representation of the MNIST dataset by building an __Autoencoder__, which comprises of:\n",
    "- an Encoder, that takes the input image and compresses it to a lower dimensional (latent) representation\n",
    "- a Decoder, which takes the output of the encoder and expands it again into the original pixel reprensentation\n",
    "\n",
    "The target is to minimize the reconstruction of this feed forward model. This is an __unsupervised__ method, which means that we do not use the labels.\n",
    "\n",
    "The latent representation shall be a vector of a few dimensions (fewer than the 28x28 original image).\n",
    "\n",
    "Hints: Play around with the activation functions (relu, leaky relu, or sigmoid), and play around with the dimension of the latent space.\n",
    "\n",
    "Additionally we want to visualize the latent representation of our data using t-SNE (can be done in tensorboard e.g. https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.... = ### FILL\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ### FILL\n",
    "        return ### FILL\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.... = ### FILL\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ### FILL\n",
    "        return ### FILL\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ### FILL\n",
    "        return ### FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, x_train)).shuffle(1024).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, x_test)).shuffle(1024).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = ### FILL\n",
    "ae = Autoencoder(dim)\n",
    "ae.build((32, 28, 28))\n",
    "ae.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "           loss=tf.keras.losses.MeanSquaredError(),\n",
    "           metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ae.fit(train_ds, epochs=5, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.epoch, history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for images, _ in train_ds.take(1):\n",
    "    n_images = len(images)\n",
    "    n_cols = 8\n",
    "    n_rows = 2*n_images // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows,n_cols, figsize=(n_cols*1.5,n_rows*1.5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    predictions = ae.call(images)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        axes[2*i].matshow(image)\n",
    "        axes[2*i].set_yticklabels([])\n",
    "        axes[2*i].set_xticklabels([])\n",
    "        axes[2*i+1].matshow(predictions[i])\n",
    "        axes[2*i+1].set_yticklabels([])\n",
    "        axes[2*i+1].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the latent representation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = ### FILL (get 1200 images and transform them to the latent represenation)\n",
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ### FILL (get the corresponding 1200 labels)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({f\"{i}\": latent[:,i] for i in range(dim)})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = tsne.pca(latent.numpy().astype(np.float64), no_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(proj[:,0], proj[:,1], c=labels, cmap=plt.get_cmap(\"tab10\"))\n",
    "plt.legend(*scatter.legend_elements())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proj = tsne.tsne(latent.numpy().astype(np.float64), max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(proj[:,0], proj[:,1], c=labels, cmap=plt.get_cmap(\"tab10\"))\n",
    "plt.legend(*scatter.legend_elements())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Tensorflow offers many ways to create models and train them:\n",
    "- More high level methods conform to the Keras specification.\n",
    "- Using lower level methods gives more control (write own model and training loop).\n",
    "\n",
    "Combinations of these are quite interoperable\n",
    "\n",
    "In detail we have learned:\n",
    "- Construct models of type `tf.keras.Model` via the sequential API, the functional API, or writing your own subclass\n",
    "- Choose loss function, optimizer, and metrics\n",
    "- Supply data directly from numpy arrays or `tf.data.Dataset`\n",
    "- Train via `model.fit()` or writing your own training loop, e.g.\n",
    "```python\n",
    "for i in range(n_epochs):\n",
    "    for xs, ys in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(xs)\n",
    "            loss = loss_object(ys, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "```\n",
    "- History of training metrics is automatically returned by `model.fit()`. In your own training loop you can do anything, e.g. fill a list of metrics yourself. (Tomorrow you'll learn about `tensorboard`)\n",
    "- Saving a model means using checkpoints, which is done via \n",
    "    - `keras.ModelCheckpoint` and `model.fit(..., callbacks=...)`, or\n",
    "    - write your own loop and use a `tf.train.Checkpoint` and `tf.train.CheckpointManager`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
