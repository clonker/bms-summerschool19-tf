{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.functions (the 'graph mode' of tensorflow)\n",
    "\n",
    "In former versions, graph and state of the graph were separate. With the new version and eager mode, this separation disappears.\n",
    "\n",
    "To recover performance, one can use `tf.function` to convert a python function into a cached tensorflow graph operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add(a, b):\n",
    "    print(\"---> Tracing: add({}, {})\".format(a, b))\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls are cached by:\n",
    "  - value if Python primitives (int, float, ...)\n",
    "  - shape and type if tensors\n",
    "  - object id otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add(tf.constant([2., 2.]), tf.constant([2., 2.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the graph is cached, no print statement\n",
    "print(add(tf.constant([7., 7.]), tf.constant([2., 2.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add(\"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primitives: call is now cached\n",
    "print(add(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also a priori specify which arguments to expect in a tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=(tf.TensorSpec(shape=(2,), dtype=tf.int32), \n",
    "                              tf.TensorSpec(shape=(2,), dtype=tf.int32)))\n",
    "def add(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add(tf.constant([2, 2]), tf.constant([3, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to print every time a tf.function is called, use the `tf.print`-operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add(a, b):\n",
    "    print(\"---> (normal print) Tracing: add({}, {})\".format(a, b))\n",
    "    tf.print(\"---> (tf print) add({}, {})\".format(a, b))\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add(tf.constant(1.), tf.constant(1.)))\n",
    "print(add(tf.constant(1.), tf.constant(1.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions with variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would behave differently in eager mode vs graph mode:\n",
    "  - in eager mode, a new variable is created in each call\n",
    "  - in graph mode, the variable would be reused until the graph is retraced\n",
    "  \n",
    "Therefore, variables cannot be created within `tf.function`-decorated methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eager mode\n",
    "\n",
    "def f(x):\n",
    "    v = tf.Variable(1.)\n",
    "    v.assign_add(x)\n",
    "    return v\n",
    "\n",
    "print(f(1.))\n",
    "print(f(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph mode\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    v = tf.Variable(1.)\n",
    "    v.assign_add(x)\n",
    "    return v\n",
    "\n",
    "# this is forbidden\n",
    "print(f(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can (due to tracing) only be created once. Making them global circumvents that problem but changes behavior compared to eager mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable(1.0)\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    return v.assign_add(x)\n",
    "\n",
    "print(f(1.))\n",
    "print(f(2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eager mode behavior can be generalized and recovered with object orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(object):\n",
    "    def __init__(self):\n",
    "        self.v = None\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        if self.v is None:\n",
    "            self.v = tf.Variable(1.)\n",
    "        self.v.assign_add(x)\n",
    "        return self.v\n",
    "    \n",
    "f = F()\n",
    "print(f(1.))\n",
    "print(f(1.))\n",
    "f = F()\n",
    "print(f(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.function` calling a eager function results in the eager function being also converted to graph operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_add(x, y):\n",
    "    print(\"tracing test\")\n",
    "    return x + y\n",
    "\n",
    "@tf.function\n",
    "def test(x, y):\n",
    "    return eager_add(x, y)\n",
    "\n",
    "# this traces eager_add only once\n",
    "print(\"graph mode:\")\n",
    "print(test(tf.constant(1.), tf.constant(2.)))\n",
    "print(test(tf.constant(1.), tf.constant(3.)))\n",
    "\n",
    "print(\"\\neager mode:\")\n",
    "# this traces eager_add eagerly\n",
    "print(eager_add(tf.constant(1.), tf.constant(2.)))\n",
    "print(eager_add(tf.constant(1.), tf.constant(3.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some more subtleties concerning loops (see, e.g., `tf.range`) and conditionals (see, e.g., `tf.cond`). A comprehensive guide can be found here: https://www.tensorflow.org/beta/tutorials/eager/tf_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing graph and training with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a clean plate\n",
    "shutil.rmtree('logs/', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a writer\n",
    "writer = tf.summary.create_file_writer('logs/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def my_func(x, y):\n",
    "    # A simple hand-rolled layer.\n",
    "    return tf.nn.relu(tf.matmul(x, y))\n",
    "\n",
    "x = tf.random.normal((3, 3))\n",
    "y = tf.random.normal((3, 3))\n",
    "\n",
    "tf.summary.trace_on(graph=True)\n",
    "out = my_func(x, y)\n",
    "with writer.as_default():   \n",
    "    tf.summary.trace_export(\n",
    "        name='my_func_trace',\n",
    "        step=0\n",
    "    )\n",
    "\n",
    "tf.summary.trace_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing scalar properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 4, num=10000)\n",
    "ys = xs**2\n",
    "ys2 = xs**3\n",
    "\n",
    "for i in tqdm.tqdm_notebook(range(len(xs))):\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('train/xsquared', ys[i], step=i)\n",
    "        tf.summary.scalar('train/xcubed', ys2[i], step=i)\n",
    "        tf.summary.scalar('test/x', xs[i], step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mean_moving_normal(mean):\n",
    "    return tf.random.normal(shape=[1000], mean=mean, stddev=1)\n",
    "\n",
    "for i in range(500):\n",
    "    normals = mean_moving_normal(tf.constant(i / 100))\n",
    "    with writer.as_default():\n",
    "        tf.summary.histogram('normals', normals, step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard can be started with `tensorboard --logdir /path/to/the/logs --port 6006` and opened in the browser under `localhost:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST variational autoencoder\n",
    "\n",
    "![title](figures/vae.png)\n",
    "\n",
    "\n",
    "Kullback Leibler divergence:\n",
    "$$\n",
    "L_{\\mathrm{KL}} = D_{\\text{KL}}(P \\,\\|\\, Q) := -\\sum_i P(i)\\log \\frac{Q(i)}{P(i)}\n",
    "$$\n",
    "In case of a VAE:\n",
    "$$\n",
    "D_\\text{KL}(\\mathcal{N}(\\mu, \\Sigma) \\,\\|\\, \\mathcal{N}(0,1))\n",
    "$$\n",
    "\n",
    "And a suitable reconstruction loss (e.g. MSE or cross entropy).\n",
    "\n",
    "Altogether:\n",
    "\n",
    "$$\n",
    "L = -\\mathbb{E}_{z\\sim q_\\Theta (z | x)}(\\log\\underbrace{ p_\\Lambda(x | z)}_{\\text{Decoder}}) + D_\\text{KL}(\\underbrace{q_\\Theta(z | x)}_{\\text{Encoder}}\\,\\|\\, p(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceNet(keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, **kw):\n",
    "        super(InferenceNet, self).__init__(**kw)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(25*25, activation=tf.nn.leaky_relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(15*15, activation=tf.nn.leaky_relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(8*8, activation=tf.nn.leaky_relu)\n",
    "                \n",
    "        self.to_mean = keras.layers.Dense(latent_dim, activation=None, name=\"to_mean\")\n",
    "        self.to_logvar = keras.layers.Dense(latent_dim, activation=None, name=\"to_logvar\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        out = self.flatten(inputs)\n",
    "        out = self.dense3(self.dense2(self.dense1(out)))\n",
    "        \n",
    "        mean = self.to_mean(out)\n",
    "        logvar = self.to_logvar(out)\n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeNet(keras.Model):\n",
    "    \n",
    "    def __init__(self, **kw):\n",
    "        super(GenerativeNet, self).__init__(**kw)\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(8*8, activation=tf.nn.leaky_relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(15*15, activation=tf.nn.leaky_relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(28 * 28, activation=tf.nn.leaky_relu)\n",
    "        \n",
    "        self.reshape = tf.keras.layers.Reshape([28, 28, 1])\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        out = self.dense3(self.dense2(self.dense1(inputs)))\n",
    "        out = self.reshape(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variational autoencoder model__:\n",
    "\n",
    "Has an inference net (encoder) and a generative net (decoder). Data is first projected onto mean and variance (inference net) and then reparameterized into the latent variable $z$. The latent variable is decoded back into the reconstruction in the full space.\n",
    "\n",
    "*Methods*:\n",
    "- `encode(data, training=None)`: encodes `data` into mean and log variance in prediction mode or training mode\n",
    "- `reparameterize(mean, logvar)`: reparameterizes $r\\sim \\mathcal{N}(0,1)$ to $z = \\mu + \\sqrt{\\exp{\\log \\sigma^2}}r$\n",
    "- `decode(z, training=None)`: decodes a latent variable $z$\n",
    "- `sample(z=None)`: generates new samples optionally based on latent variable $z$\n",
    "- `call(inputs, training=None)`: chained encoding, reparameterizing, decoding\n",
    "- `compute_loss(x, training=None)`: computes the loss for a batch of data `x`\n",
    "- `compute_apply_gradients(x, optimizer)`: computes and applies gradients, model update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, *args, **kw):\n",
    "        super(VAE, self).__init__(*args, **kw)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = InferenceNet(self.latent_dim, name=\"Encoder\")\n",
    "        self.generative_net = GenerativeNet(name=\"Decoder\")\n",
    "        \n",
    "    @tf.function\n",
    "    def encode(self, x, training=None):\n",
    "        return self.inference_net(x, training=training)\n",
    "\n",
    "    @tf.function\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        with tf.name_scope(\"reparameterize\"):\n",
    "            rnd = tf.random.normal(shape=tf.shape(mean))\n",
    "            z = rnd * tf.exp(logvar * .5) + mean\n",
    "        return z\n",
    "\n",
    "    @tf.function\n",
    "    def decode(self, z, training=None):\n",
    "        return self.generative_net(z, training=training)\n",
    "    \n",
    "    @tf.function\n",
    "    def sample(self, z=None):\n",
    "        with tf.name_scope(\"sample\"):\n",
    "            if z is None:\n",
    "                z = tf.random.normal(shape=(100, self.latent_dim))\n",
    "            output = self.decode(z, training=False)\n",
    "        return output\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        mean, logvar = self.encode(inputs, training=training)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        output = self.decode(z, training=training)\n",
    "        return output\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_loss(self, x, training=None):\n",
    "        with tf.name_scope(\"vae_loss\"):\n",
    "            mean, logvar = self.encode(x, training=training)\n",
    "            z = self.reparameterize(mean, logvar)\n",
    "            x_logit = self.decode(z, training=training)\n",
    "            \n",
    "            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "            \n",
    "            with tf.name_scope(\"KL\"):\n",
    "                kl = -.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar), axis=1)\n",
    "            loss = tf.reduce_mean(kl + tf.reduce_sum(reconstruction_loss, axis=[1, 2, 3]))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_apply_gradients(self, x, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "x_test = np.array(x_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(x_train)\\\n",
    "            .map(lambda x: tf.expand_dims(x, -1)).shuffle(1024).batch(512)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(x_test)\\\n",
    "            .map(lambda x: tf.expand_dims(x, -1)).batch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in tqdm.tqdm_notebook(range(50)):\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    for x in train_ds:\n",
    "        l = model.compute_apply_gradients(x, optimizer)\n",
    "        train_loss(l)\n",
    "    train_losses.append(train_loss.result())\n",
    "        \n",
    "    test_loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_ds:\n",
    "        test_loss(model.compute_loss(test_x))\n",
    "    test_losses.append(test_loss.result())\n",
    "        \n",
    "    print('Epoch: {}, Train loss: {}, Test loss: {}'.format(epoch + 1, train_loss.result(), \n",
    "                                                            test_loss.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images in train_ds.take(1):\n",
    "    images = images[:8*4]\n",
    "    n_images = len(images)\n",
    "    n_cols = 8\n",
    "    n_rows = 2*n_images // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows,n_cols, figsize=(n_cols*1.5,n_rows*1.5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    predictions = model(images)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        axes[2*i].imshow(image.numpy().squeeze())\n",
    "        axes[2*i].set_yticklabels([])\n",
    "        axes[2*i].set_xticklabels([])\n",
    "        axes[2*i+1].imshow(predictions[i].numpy().squeeze())\n",
    "        axes[2*i+1].set_yticklabels([])\n",
    "        axes[2*i+1].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The swiss roll jump process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./figures/swiss-roll-process.png)\n",
    "![title](figures/tae.png)\n",
    "<cite>Wehmeyer, Noé, (2018). Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics. J. Chem. Phys., 148(24), 241703.</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dimredux-challenge.npz\", \"rb\") as f:\n",
    "    X = np.load(f)\n",
    "    data = X['data']\n",
    "    dtraj = X['dtraj']\n",
    "    \n",
    "print(\"got data with shape {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax = f.add_subplot(111, projection='3d')\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=data[:, 2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "ax = f.add_subplot(111, projection='3d')\n",
    "stride=50\n",
    "ax.scatter(*(data[::stride].T), c=dtraj[::stride]/np.max(dtraj), alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = tf.linspace(-4., 4., 50)\n",
    "with tf.GradientTape() as tape:\n",
    "    # xs is not trainable so we need to watch it\n",
    "    tape.watch(xs)\n",
    "    ys = swish(xs)\n",
    "dys_dxs = tape.gradient(ys, xs)\n",
    "\n",
    "plt.plot(xs, ys, label='swish')\n",
    "plt.plot(xs, dys_dxs, label='swish derivative')\n",
    "plt.plot(np.linspace(-5, 5, num=50), np.zeros((50,)), 'k--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data[:-1].astype(np.float32), data[1:].astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = int(len(data) * .1)\n",
    "ds_test = dataset.take(n_val).batch(batch_size)\n",
    "ds_val = dataset.skip(n_val).take(n_val).batch(batch_size)\n",
    "# note the shuffle on the batches not the data itself!\n",
    "ds_train = dataset.skip(2*n_val).batch(batch_size).shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceNet(keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, **kw):\n",
    "        super(InferenceNet, self).__init__(**kw)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        ### you can experiment with different activations and layer types here\n",
    "        self.activation = keras.layers.Lambda(lambda x: swish(x), name=\"swish\")\n",
    "        \n",
    "        self.dense1 = keras.layers.Dense(300, name=\"dense1\")\n",
    "        self.dropout1 = keras.layers.Dropout(rate=0.2, name=\"dropout1\")\n",
    "        \n",
    "        self.dense2 = keras.layers.Dense(150, activation=swish, name=\"dense2\")\n",
    "        self.dropout2 = keras.layers.Dropout(rate=0.2, name=\"dropout2\")\n",
    "        \n",
    "        self.to_mean = keras.layers.Dense(latent_dim, activation=None, name=\"to_mean\")\n",
    "        self.to_logvar = keras.layers.Dense(latent_dim, activation=None, name=\"to_logvar\")\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        ### FILL\n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeNet(keras.Model):\n",
    "    \n",
    "    def __init__(self, **kw):\n",
    "        super(GenerativeNet, self).__init__(**kw)\n",
    "        \n",
    "        ### you can experiment with different activations layer types here\n",
    "        self.activation = keras.layers.Lambda(lambda x: swish(x), name=\"swish\")\n",
    "        \n",
    "        self.dense1 = keras.layers.Dense(150, activation=swish, name=\"dense1\")\n",
    "        self.dropout1 = keras.layers.Dropout(rate=0.2, name=\"dropout1\")\n",
    "        self.dense2 = keras.layers.Dense(300, activation=swish, name=\"dense2\")\n",
    "        self.dropout2 = keras.layers.Dropout(rate=0.2, name=\"dropout2\")\n",
    "        self.dense3 = keras.layers.Dense(3, activation=None, name=\"dense3\")\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        ### FILL\n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVAE(keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, *args, **kw):\n",
    "        super(TVAE, self).__init__(*args, **kw)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = InferenceNet(self.latent_dim, name=\"Encoder\")\n",
    "        self.generative_net = GenerativeNet(name=\"Decoder\")\n",
    "        \n",
    "    @tf.function\n",
    "    def sample(self, z=None):\n",
    "        with tf.name_scope(\"sample\"):\n",
    "            if z is None:\n",
    "                z = tf.random.normal(shape=(100, self.latent_dim))\n",
    "            output = self.decode(z, apply_sigmoid=True, training=False)\n",
    "        return output\n",
    "    \n",
    "    def encode(self, x, training=True):\n",
    "        return self.inference_net(x, training=training)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        with tf.name_scope(\"reparameterize\"):\n",
    "            eps = tf.random.normal(shape=tf.shape(mean))\n",
    "            z = eps * tf.exp(logvar * .5) + mean\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False, training=None):\n",
    "        logits = self.generative_net(z, training=training)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        mean, logvar = self.encode(inputs, training=training)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        output = self.decode(z, training=training)\n",
    "        return output\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_loss(self, x, x_lagged, training=None):\n",
    "        with tf.name_scope(\"vae_loss\"):\n",
    "            # FILL: encode, reparameterize, decode, implement MSE reconstruction loss and KL\n",
    "            # return loss\n",
    "            ...\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_apply_gradients(self, x, x_lagged, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x, x_lagged)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TVAE(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm.tqdm_notebook(range(n_epochs)):\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    for x, x_lagged in ds_train:\n",
    "        batch_loss = model.compute_apply_gradients(x, x_lagged, optimizer)\n",
    "        train_loss(batch_loss)\n",
    "    train_losses.append(train_loss.result())\n",
    "    \n",
    "    val_loss = tf.keras.metrics.Mean()\n",
    "    for val_x, val_x_lagged in ds_val:\n",
    "        val_loss(model.compute_loss(val_x, val_x_lagged, training=False))\n",
    "    val_losses.append(val_loss.result())\n",
    "    \n",
    "    print('Epoch: {}, Train loss: {}, Val loss: {}'\n",
    "          .format(epoch + 1, train_loss.result(), val_loss.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = []\n",
    "for X, Y in ds_test:\n",
    "    mu, logvar = model.encode(X, training=False)\n",
    "    z = model.reparameterize(mu, logvar)\n",
    "    zs.append(z.numpy().squeeze())\n",
    "z = np.concatenate(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cc = KMeans(n_clusters=4).fit(z[:, None]).cluster_centers_\n",
    "\n",
    "f, ax = plt.subplots(1,1,figsize=(15, 5))\n",
    "ax.plot(z[:1000])\n",
    "ax.hlines(cc, xmin=0, xmax=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing graph and training progress to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('logs/', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_dir = 'logs/swissroll/train'\n",
    "val_log_dir = 'logs/swissroll/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TVAE(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = tf.random.normal(shape=(100, 3))\n",
    "yrec = tf.random.normal(shape=(100, 3))\n",
    "\n",
    "tf.summary.trace_on(graph=True)\n",
    "l = model.compute_loss(y, yrec)\n",
    "with train_summary_writer.as_default():   \n",
    "    tf.summary.trace_export(\n",
    "        name='tvae2_graph',\n",
    "        step=0\n",
    "    )\n",
    "\n",
    "tf.summary.trace_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'logs/ckpt/'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=2)\n",
    "checkpoint.restore(checkpoint_manager.latest_checkpoint);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm.tqdm_notebook(range(n_epochs)):\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    for x, x_lagged in ds_train:\n",
    "        l = model.compute_apply_gradients(x, x_lagged, optimizer)\n",
    "        train_loss(l)\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "    \n",
    "    val_loss = tf.keras.metrics.Mean()\n",
    "    for val_x, val_x_lagged in ds_val:\n",
    "        val_loss(model.compute_loss(val_x, val_x_lagged))\n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_loss.result(), step=epoch)\n",
    "    \n",
    "    checkpoint_manager.save()\n",
    "    \n",
    "    print('Epoch: {}, Train loss: {}, Val loss: {}'.format(epoch + 1, train_loss.result(), val_loss.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = []\n",
    "for X, Y in ds_test:\n",
    "    mu, logvar = model.encode(X, training=False)\n",
    "    z = model.reparameterize(mu, logvar)\n",
    "    zs.append(z.numpy().squeeze())\n",
    "z = np.concatenate(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z[:1000, 0], z[:1000, 1], c=dtraj[:1000] / np.max(dtraj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=4).fit(z)\n",
    "z_assignments = km.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z[:1000, 0], z[:1000, 1], c=z_assignments[:1000] / np.max(z_assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
